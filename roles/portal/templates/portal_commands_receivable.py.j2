{% if ansible_python_interpreter == '/usr/bin/env python3-docker' %}
{# This is a workaround for having to call the executable itself instead of the shell wrapper script that ansible-docker creates #}
#!/usr/local/lib/docker/virtualenv/bin/python3 -u
{% else %}
#!/usr/bin/env python3
{% endif %}

import os
import docker
import socket
import shutil
import subprocess
from datetime import datetime
from ast import literal_eval


def get_no_cache(build_target, collection_version, oc_mgmt_branch):
    # Get the project git repo's last commit time
    cmd1 = f"git clone --branch {oc_mgmt_branch} --depth=1 https://gitlab.com/"
    cmd2 = "compositionalenterprises/ansible-project-ourcompose_management.git"
    project_dir = '/tmp/ansible_project'
    subprocess.run(cmd1 + cmd2 + ' ' + project_dir, shell=True)
    cmd1 = "git log -1 --date=short --pretty=format:%ci | cut -d ' ' -f -2"
    git_proj_timestamp = subprocess.check_output(cmd1, shell=True,
        universal_newlines=True, cwd=project_dir).strip()
    # Cleanup git repo
    subprocess.run("rm -rf {}".format(project_dir), shell=True)

    # Get the collection's git repo's last commit time
    cmd1 = f"git clone --branch {collection_version} --depth=1 "
    cmd2 = "https://gitlab.com/compositionalenterprises/ansible-collection-"
    cmd3 = "compositionalenterprises.ourcompose.git"
    collection_dir = '/tmp/ansible_collection'
    subprocess.run(cmd1 + cmd2 + cmd3 + ' ' + collection_dir, shell=True)
    cmd1 = "git log -1 --date=short --pretty=format:%ci | cut -d ' ' -f -2"
    git_coll_timestamp = subprocess.check_output(cmd1, shell=True,
        universal_newlines=True, cwd=collection_dir).strip()
    # Cleanup git repo
    subprocess.run("rm -rf {}".format(collection_dir), shell=True)

    # Get the docker image's last build time
    try:
        cmd1 = "docker inspect -f '{% raw %}{{.Created}}{% endraw %}' "
        cmd2 = f"{build_target}/commands_receivable:{collection_version} "
        docker_timestamp = subprocess.check_output(cmd1 + cmd2, shell=True,
            universal_newlines=True)
        docker_timestamp = docker_timestamp.replace('T', ' ').split('.')[0]
    except subprocess.CalledProcessError as e:
        # Hitting this means that we don't have an image like this built
        print("Could not find any matching docker image...")
        return True

    git_proj_datetime = datetime.strptime(git_proj_timestamp,
                                          '%Y-%m-%d %H:%M:%S')
    git_coll_datetime = datetime.strptime(git_coll_timestamp,
                                          '%Y-%m-%d %H:%M:%S')
    docker_datetime = datetime.strptime(docker_timestamp, '%Y-%m-%d %H:%M:%S')

    # Greater than (`>`) when comparing datetime objects means "newer than".
    # So here we are testing if the git history has a commit that is newer
    # than the time that the docker image was built. This means that we want to
    # build a completely new container, and 
    if (git_proj_datetime > docker_datetime or
            git_coll_datetime > docker_datetime):
        print("Found git date that is later than our docker image build date.")
        return True

    print("No need to bypass the cache. Project and Collection up-to-date.")
    return False


def build_command(spec):
    """
    Spec should come in with at least one key, 'script'. This is the path to
    the script that we should run. That will also let us determine whether we
    are going to be running a python script or an ansible playbook.
    """
    # Start the command string here
    command = ''

    # Here we add the ansible-playbook command if it's an ansible run.
    # Otherwise, all of our python scripts have their shebang set up.
    #
    # Also, we add the format for the extra args
    if spec['script'].split('/')[0] == 'playbooks':
        cmd1 = 'ansible-playbook --private-key ~/.ssh/commands_receivable -i '
        cmd2 = 'environment/localhost.yml '
        command =  cmd1 + cmd2
        args_format = " -e {}={}"
    else:
        # This requires us to choose whether we enforce flag or switch passing
        # of arguments. At this point, since flags are going to be more
        # descriptive, let's use those.
        args_format = " --{} {}"

    # Add the script name
    command = command + spec['script']

    # Parse the args
    if 'args' in spec:
        for arg in spec['args']:
            command = command + args_format.format(arg, spec['args'][arg])

    command = command.split(' ')

    return command


def set_entrypoint_path():
    # Set the script dict for the options we have to choose amongst.
    # We're now echoing localhost into an inventory file. This allows us to
    # pick up the group_vars directory that is inside of the environment/ dir.
    lines = [
        '#!/bin/bash -e\n',
        "ln -sT /portal_storage/ansible/environment "
        "/var/ansible/environment\n",
        "cd /var/ansible\n",
        "echo 'all:' > environment/localhost.yml\n",
        "echo '  children:' >> environment/localhost.yml\n",
        "echo '    compositional:' >> environment/localhost.yml\n",
        "echo '      hosts:' >> environment/localhost.yml\n",
        "echo '        localhost:' >> environment/localhost.yml\n",
        "echo $VAULT_PASSWORD > environment/.vault_pass\n",
        'exec "$@"\n'
        ]

    # Write out a demo entrypoint script
    os.makedirs('/tmp/entrypoint', exist_ok=True)
    entrypoint_path = '/tmp/entrypoint/entrypoint.sh'
    with open(entrypoint_path, 'w') as entrypoint_script:
        for line in lines:
            entrypoint_script.write(line)

    st = os.stat(entrypoint_path)
    os.chmod(entrypoint_path, 0o755)

    return entrypoint_path

def build_container_image(collection_version, build_target):
    os.makedirs('/tmp/docker_build', exist_ok=True)
    dockerfile_path = '/tmp/docker_build/Dockerfile'
    
    #
    # Get the ourcompose_management project branch to use
    # We did this before, but only for determining whether to build the cache
    # or not. Now we do it for that and the dockerfile which builds the
    # container.
    #
    if collection_version.startswith('v'):
        # This gets the stable major version that the tag is targeting
        # from the string like 'v3.2.1'
        oc_mgmt_branch = 'stable-' + collection_version[1]
        oc_mgmt_maj_ver = collection_version[1]
    elif collection_version.startswith('stable'):
        # This gets the stable major version from the tag that is
        # targeting the 'stable-3.2' type string
        oc_mgmt_branch = 'stable-' + collection_version.split('-')[1][0]
        oc_mgmt_maj_ver = collection_version.split('-')[1][0]
    else:
        # This is most likely a development branch, so let's figure out if
        # there is a corresponding project branch to the collection branch
        # We check to see if there is a corresponding project branch for the
        # collection branch, if the name of the branch doesn't start with
        # stable- or v. (e.g., it's an oc#### branch). If there is none,
        # we take a look at the galaxy.yml and see what major version of
        # the project (stable-#) is going to be compatible with this branch,
        # and use that one when building the dockerfile/checking for no_cache.

        # Get the collection's git repo galaxy version
        cmd1 = f"git clone --branch {collection_version} --depth=1 "
        cmd2 = "https://gitlab.com/compositionalenterprises/ansible-collection-"
        cmd3 = "compositionalenterprises.ourcompose.git"
        collection_dir = '/tmp/ansible_collection'
        subprocess.run(cmd1 + cmd2 + cmd3 + ' ' + collection_dir, shell=True)
        cmd1 = "grep 'version:' galaxy.yml | cut -d ' ' -f 2 | cut -d '.' -f 1"
        git_coll_maj_version = subprocess.check_output(cmd1, shell=True,
            universal_newlines=True, cwd=collection_dir).strip()
        # Cleanup git repo
        subprocess.run("rm -rf {}".format(collection_dir), shell=True)
        oc_mgmt_maj_ver = git_coll_maj_version

        # Get the presence of an upstream branch that matches the collection
        cmd1 = "git clone https://gitlab.com/"
        cmd2 = "compositionalenterprises/"
        cmd3 = "ansible-project-ourcompose_management.git "
        project_dir = '/tmp/ansible_project'
        subprocess.run(cmd1 + cmd2 + cmd3 + project_dir, shell=True)
        cmd1 = "git branch -a | "
        cmd2 = f"grep 'remotes/origin/{collection_version}'"
        git_proj_matching_branch = subprocess.run(cmd1 + cmd2,
            shell=True, cwd=project_dir)
        # Cleanup git repo
        subprocess.run("rm -rf {}".format(project_dir), shell=True)

        # Set the management branch after deriving the above
        if git_proj_matching_branch.returncode == 0:
            # This branch exists in the project, so let's set that one
            # specifically
            oc_mgmt_branch = collection_version
        else:
            # This branch does NOT exists in the project, so let's just use the
            # matching major version stable instance
            oc_mgmt_branch = 'stable-' + git_coll_maj_version

    # We set the ansible version to install in the docker file based on the
    # major version, however it's derived above
    if oc_mgmt_maj_ver == '3':
        ansible_ver = '>=2.10,<2.11'
    elif oc_mgmt_maj_ver == '4':
        ansible_ver = '>=4,<5'

    with open(dockerfile_path, 'w') as dockerfile:
        dockerfile.write(
            "FROM ubuntu:20.04\n"
            "ENV DEBIAN_FRONTEND=noninteractive\n"
            "RUN apt-get update && \\\n"
            "    apt-get install -y "
            "git dnsutils python3 libffi-dev libssl-dev python3-dev "
            "python3-distutils python3-pip && \\\n"
            f"    pip3 install 'ansible{ansible_ver}' ansible-vault requests "
            "tabulate packaging && \\\n"
            f"    git clone --branch {oc_mgmt_branch} "
            "https://gitlab.com/compositionalenterprises/"
            "ansible-project-ourcompose_management.git /var/ansible\n"
            "WORKDIR /var/ansible \n"
            "RUN sed -i 's/, plays@.\/.vault_pass//' ansible.cfg && \\\n"
            "    rm -rf playbooks/group_vars/ && \\\n"
            "    sed -i 's/version: master/version: "
            f"{collection_version}/' requirements.yml && \\\n"
            "    ansible-galaxy install -fr requirements.yml\n"
            )

    no_cache = get_no_cache(build_target, collection_version, oc_mgmt_branch)
    client = docker.from_env()
    # TODO: Use buildargs instead of f-string above?
    print("Building container...")
    container_image = client.images.build(
            path='/tmp/docker_build',
            tag="{}/commands_receivable:{}".format(
                build_target, collection_version),
            pull=True,
            nocache = no_cache,
            )
    for line in container_image[1]:
        print(line)

    shutil.rmtree('/tmp/docker_build')

    return container_image

def get_container_image(spec):
    # TODO: Test for image present:
    #
    #   ➜  ~ docker images -q mariadb:latest
    #        e76a4b2ed1b4
    #   ➜  ~ docker images -q mariadb:1.10
    #   ➜  ~ echo $?
    #   0

    # TODO: Test for spec passed something for us to auth to the docker
    # registry with
    try:
        client = docker.from_env()
        client.images.pull(
                repository='compositionalenterprises/commands_receivable',
                tag=spec['collection_version']
                )
        return 'compositionalenterprises/commands_receivable:{}'.format(
                spec['collection_version'])
    except docker.errors.APIError:
        build_container_image(spec['collection_version'], 'local')
        return "local/commands_receivable:{}".format(
                spec['collection_version'])


def run_docker_command(spec):
    """
    Takes the spec that the server passes us and runs docker-compose based off
    of it.
    """
    client = docker.from_env()
    set_entrypoint_path()
    print('Running Container')
    # TODO Deal with local/remove pathing
    container = client.containers.run(
        image=get_container_image(spec),
        command=build_command(spec),
        entrypoint='/entrypoint/entrypoint.sh',
        network_mode='host',
        detach=True,
        stream=True,
        environment={
            'VAULT_PASSWORD': spec['vault_password']
            },
        volumes={
            '/srv/local/portal_storage/': {
                'bind': '/portal_storage',
                'mode': 'rw'
                },
            '/root/.ssh/': {
                'bind': '/root/.ssh',
                'mode': 'ro'
                },
            '/tmp/entrypoint/': {
                'bind': '/entrypoint',
                'mode': 'ro'
                }
            },
        remove=True
        )

    return container

def systemd_socket_response():
    """
    Accepts every connection of the listen socket provided by systemd, send the
    HTTP Response 'OK' back.
    """
    try:
        fds = listen_fds()
    except ImportError:
        fds = [3]

    for fd in fds:
        with socket.fromfd(fd, socket.AF_INET, socket.SOCK_STREAM) as sock:
            sock.settimeout(0)

            try:
                conn, addr = sock.accept()
                with conn:
                    fragments = []
                    while True:
                        data = conn.recv(1024)
                        if not data:
                            break
                        fragments.append(data)
                    # We are expecting at least the following here:
                    #
                    # {
                    #     'vault_password': str(), # Required
                    #     'script': str(), # Required
                    #     'args': dict(), # Optional
                    #     'collection_version': str(), # Required
                    # }
                    spec_bytes = b''.join(fragments)
                    spec = literal_eval(spec_bytes.decode('utf8'))
                    conn.sendall(b'Executing...')
                    print('Executing...')
                    container = run_docker_command(spec)
                    container_logs = container.logs(stream=True, follow=True)
                    try:
                        while True:
                            line = next(container_logs)
                            if line == b'\n':
                                continue
                            print(line)
                            conn.send(line)
                    except StopIteration:
                        pass
                    conn.sendall(b'Executed...')
                    print('Executed...')
            except socket.timeout:
                pass
            except OSError as e:
                # Connection closed again? Don't care, we just do our job.
                print(e)

if __name__ == "__main__":
    from systemd.daemon import listen_fds;
    if os.environ.get("LISTEN_FDS", None) != None:
        systemd_socket_response()
